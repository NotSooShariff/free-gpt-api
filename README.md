# GPT-Tokenizer API

## Overview

This GPT-Tokenizer API is a personal side project which I developed because I was running out of GPT tokens for use in various projects via the API xD. This is based on Selenium, which allows it to make requests through a browser and provide responses in a format usable by other APIs. Please note that this project is still under development and may have limitations.

## Quick Links

- [GPT-Tokenizer API](#gpt-tokenizer-api)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
  - [Usage](#usage)
  - [Solutions](#solutions)
  - [Directory Structure](#directory-structure)
  - [Contributing](#contributing)
  - [License](#license)

## Getting Started

### Prerequisites

Before running the GPT-Tokenizer API, you'll need the following prerequisites:

- Python 3.x
- Selenium
- Chrome WebDriver
- Ngrok (optional, for web access)

### Installation

1. Clone the repository to your local machine:

```bash
git clone https://github.com/NotSooShariff/free-gpt-api.git
```

2. Install the required Python packages:

```bash
cd free-gpt-api
pip install -r requirements.txt
```

## Usage

To run the GPT-Tokenizer API, follow these steps:

1. Navigate to the API directory:

```bash
cd api
```

2. Run the `app.py` script:

```bash
python app.py
```

3. In another terminal, you can make requests to the API using the provided `apitest.py` script located in the `testcode` directory:

```bash
cd ../testcode
python apitest.py
```

## Solutions

As mentioned earlier, due to the way Selenium works (by popping up a browser locally), containerizing this project has been challenging. However, there are a few ways to use this project:

1. **Ngrok Tunneling**: You can use Ngrok to open a tunnel to your computer's port and make the API accessible via an external URL. This allows you to use the API remotely.

2. **Local Server**: You can run the API on your local server and use it locally. This is suitable for testing and development purposes.

## Directory Structure

The repository is structured as follows:

- `__pycache__/`: Cache files generated by Python.
- `api/`: Contains the API code.
  - `app.py`: The main API script.
- `testcode/`: Contains scripts for testing the API.
  - `apitest.py`: A script for testing API requests.
- `app.yaml`: Configuration file (if applicable).
- `Dockerfile`: Dockerfile for containerization (work in progress).
- `requirements.txt`: List of required Python packages.

## Contributing

Contributions to this project are welcome! Feel free to open issues, suggest improvements, or submit pull requests. Please review our [contribution guidelines](CONTRIBUTING.md) for more details.

## License

This project is licensed under the [MIT License](LICENSE).

---

Enjoy using the GPT-Tokenizer API! If you encounter any issues or have suggestions for improvements, please don't hesitate to reach out.
